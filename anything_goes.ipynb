{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFmGHgdTvu6NKakN403OgD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreihMurray/NLP-4/blob/master/anything_goes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "tbsVlxPxKnHT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "import keras\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from math import log2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEtePjF8K85C",
        "outputId": "8afc1938-3c15-4f89-bad0-d465e929a7f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(grams, raw_data):\n",
        "    chars = sorted(list(set(raw_data)))\n",
        "    mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "    sequences = list()\n",
        "    for line in tqdm(grams, desc='Encoding'):\n",
        "        # integer encode line\n",
        "        encoded_seq = [mapping[char] for char in line]\n",
        "        # store\n",
        "        sequences.append(encoded_seq)\n",
        "    return sequences, mapping"
      ],
      "metadata": {
        "id": "3hU8ifQOKsZ1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab, 20, input_length=GRAMS-1, trainable=True))\n",
        "    model.add(GRU(25, recurrent_dropout=0.1, dropout=0.1))\n",
        "    model.add(Dense(vocab, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "4MffHLMrKtsc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_name):\n",
        "    f = open(file_name, \"r\")\n",
        "\n",
        "    full_text = f.read().split(\" \")\n",
        "    split_data = []\n",
        "\n",
        "    for line in full_text: #tqdm(full_text, desc='Splitting words'):\n",
        "        split_data.append(line.lower().strip())\n",
        "\n",
        "    train = ' '.join(split_data[:int(len(split_data) * 0.8)])\n",
        "    test = ' '.join(split_data[int(len(split_data) * 0.8):])\n",
        "\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "cOiW3aNoK1Q_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_test_data(file_name):\n",
        "    f = open(file_name, \"r\")\n",
        "\n",
        "    full_text = f.read().split(\" \")\n",
        "    split_data = []\n",
        "\n",
        "    for line in full_text: #tqdm(full_text, desc='Splitting words'):\n",
        "        split_data.append(line.lower().strip())\n",
        "\n",
        "    return ' '.join(split_data)"
      ],
      "metadata": {
        "id": "2bjtzkHoYPrg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_n_grams(data, n=3):\n",
        "    descript = \"Generating \" + str(n) + \" Grams:\"\n",
        "\n",
        "    n_grams = [''.join(data[i:i+n]) for i in tqdm(range(len(data) - n + 1), desc=descript)]\n",
        "\n",
        "    return n_grams"
      ],
      "metadata": {
        "id": "0iqsefDGK40y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    data, hold_out = read_file('/content/gdrive/MyDrive/Colab_Notebooks/NLP/swahili/cwe-train.txt')\n",
        "\n",
        "    n_grams = gen_n_grams(data, GRAMS)\n",
        "    n_grams, mapping = encode(n_grams, data)\n",
        "\n",
        "    vocab = len(mapping)\n",
        "    sequences = np.array(n_grams)\n",
        "    # create X and y\n",
        "    x, y = sequences[:, :-1], sequences[:, -1]\n",
        "    # one hot encode y\n",
        "    y = to_categorical(y, num_classes=vocab)\n",
        "    # create train and validation sets\n",
        "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=0.1, random_state=42)\n",
        "\n",
        "    print('Train shape:', x_tr.shape, 'Val shape:', x_val.shape)\n",
        "\n",
        "    model = build_model(vocab)\n",
        "\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=5)\n",
        "\n",
        "    model.fit(x_tr, y_tr, epochs=10, verbose=1, validation_data=(x_val, y_val), callbacks=stop_early, batch_size=250)\n",
        "    model.save('/content/gdrive/MyDrive/Colab_Notebooks/NLP/swahili/act_model')\n"
      ],
      "metadata": {
        "id": "T_2m3XwmKwSm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GRAMS = 12"
      ],
      "metadata": {
        "id": "wmxZN9ZdLO5z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMsCOFTvKzFn",
        "outputId": "2760e491-973f-4412-e2fc-f0b72a1ace77"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 12 Grams:: 100%|██████████| 482805/482805 [00:00<00:00, 1027639.76it/s]\n",
            "Encoding: 100%|██████████| 482805/482805 [00:01<00:00, 309078.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (434524, 11) Val shape: (48281, 11)\n",
            "Epoch 1/10\n",
            "1739/1739 [==============================] - 38s 20ms/step - loss: 2.0845 - acc: 0.3514 - val_loss: 1.8307 - val_acc: 0.4114\n",
            "Epoch 2/10\n",
            "1739/1739 [==============================] - 40s 23ms/step - loss: 1.8084 - acc: 0.4179 - val_loss: 1.7293 - val_acc: 0.4415\n",
            "Epoch 3/10\n",
            "1739/1739 [==============================] - 45s 26ms/step - loss: 1.7490 - acc: 0.4378 - val_loss: 1.6800 - val_acc: 0.4594\n",
            "Epoch 4/10\n",
            "1739/1739 [==============================] - 43s 25ms/step - loss: 1.7160 - acc: 0.4477 - val_loss: 1.6473 - val_acc: 0.4704\n",
            "Epoch 5/10\n",
            "1739/1739 [==============================] - 44s 25ms/step - loss: 1.6946 - acc: 0.4553 - val_loss: 1.6262 - val_acc: 0.4742\n",
            "Epoch 6/10\n",
            "1739/1739 [==============================] - 37s 21ms/step - loss: 1.6789 - acc: 0.4611 - val_loss: 1.6083 - val_acc: 0.4860\n",
            "Epoch 7/10\n",
            "1739/1739 [==============================] - 48s 28ms/step - loss: 1.6669 - acc: 0.4651 - val_loss: 1.5952 - val_acc: 0.4869\n",
            "Epoch 8/10\n",
            "1739/1739 [==============================] - 35s 20ms/step - loss: 1.6574 - acc: 0.4680 - val_loss: 1.5833 - val_acc: 0.4911\n",
            "Epoch 9/10\n",
            "1739/1739 [==============================] - 35s 20ms/step - loss: 1.6494 - acc: 0.4704 - val_loss: 1.5747 - val_acc: 0.4952\n",
            "Epoch 10/10\n",
            "1739/1739 [==============================] - 47s 27ms/step - loss: 1.6427 - acc: 0.4736 - val_loss: 1.5674 - val_acc: 0.4941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_entropy(preds):\n",
        "    entropy = 0\n",
        "\n",
        "    count = 0\n",
        "    pred_len = len(preds)\n",
        "\n",
        "    for row in tqdm(preds, desc='Calculating Entropy'):\n",
        "      entropy -= (1/(pred_len)) * log2(max(row))\n",
        "\n",
        "    return entropy"
      ],
      "metadata": {
        "id": "FdaJ8Y9nb4lk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_my_model():\n",
        "    model = keras.models.load_model('/content/gdrive/MyDrive/Colab_Notebooks/NLP/swahili/act_model')\n",
        "\n",
        "    data = read_test_data('/content/gdrive/MyDrive/Colab_Notebooks/NLP/swahili/cwe-train.txt')\n",
        "\n",
        "    data = data[int(len(data)*0.7):]\n",
        "\n",
        "    n_grams = gen_n_grams(data, GRAMS-1)\n",
        "    n_grams, mapping = encode(n_grams, data)\n",
        "\n",
        "    vocab = len(mapping)\n",
        "    sequences = np.array(n_grams)\n",
        "\n",
        "    preds = model.predict(sequences)\n",
        "    \n",
        "    entropy = calc_entropy(preds)\n",
        "    print('\\n', entropy)\n",
        "    "
      ],
      "metadata": {
        "id": "n2e5zjJbX4Sw"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_my_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWVPdo1DZaHx",
        "outputId": "2247f149-eae9-4385-94c6-501cc3935bd5"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 11 Grams:: 100%|██████████| 181020/181020 [00:00<00:00, 1032077.41it/s]\n",
            "Encoding: 100%|██████████| 181020/181020 [00:00<00:00, 299971.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5657/5657 [==============================] - 19s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Entropy: 100%|██████████| 181020/181020 [00:00<00:00, 248707.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 1.2353199400424082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}