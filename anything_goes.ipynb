{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwgWvl3Ti13ycgtVpIywRY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreihMurray/NLP-4/blob/master/anything_goes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "tbsVlxPxKnHT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "import keras\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from math import log2\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEtePjF8K85C",
        "outputId": "044454e4-38af-44ed-b90b-36f5d8dc4a19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heavily based on https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/#h2_7"
      ],
      "metadata": {
        "id": "Ljp9_IHfenhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(grams, raw_data, loader=False):\n",
        "    chars = sorted(list(set(raw_data)))\n",
        "    mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "    if loader == True:\n",
        "        with open('/content/gdrive/MyDrive/Colab_Notebooks/NLP/swahili/swmap.json') as infile:\n",
        "          mapping = json.load(infile) \n",
        "\n",
        "    sequences = list()\n",
        "    for line in tqdm(grams, desc='Encoding'):\n",
        "        # integer encode line\n",
        "        encoded_seq = [mapping[char] for char in line]\n",
        "        # store\n",
        "        sequences.append(encoded_seq)\n",
        "    return sequences, mapping"
      ],
      "metadata": {
        "id": "3hU8ifQOKsZ1"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on code from https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/#h2_7"
      ],
      "metadata": {
        "id": "B9gsORg7ergF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab, 20, input_length=GRAMS-1, trainable=True))\n",
        "    model.add(GRU(25, recurrent_dropout=0.1, dropout=0.1))\n",
        "    model.add(Dense(vocab, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "4MffHLMrKtsc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "original"
      ],
      "metadata": {
        "id": "MxE5hjPTetSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_name):\n",
        "    f = open(file_name, \"r\")\n",
        "\n",
        "    full_text = f.read().split(\" \")\n",
        "    split_data = []\n",
        "\n",
        "    for line in full_text: #tqdm(full_text, desc='Splitting words'):\n",
        "        split_data.append(line.lower().strip())\n",
        "\n",
        "    train = ' '.join(split_data[:int(len(split_data) * 0.3)])\n",
        "    test = ' '.join(split_data[int(len(split_data) * 0.5):])\n",
        "\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "cOiW3aNoK1Q_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "original"
      ],
      "metadata": {
        "id": "8n6xm5Ezeuqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_test_data(file_name):\n",
        "    f = open(file_name, \"r\")\n",
        "\n",
        "    full_text = f.read().split(\" \")\n",
        "    split_data = []\n",
        "\n",
        "    for line in full_text: #tqdm(full_text, desc='Splitting words'):\n",
        "        split_data.append(line.lower().strip())\n",
        "\n",
        "    return ' '.join(split_data)"
      ],
      "metadata": {
        "id": "2bjtzkHoYPrg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "original"
      ],
      "metadata": {
        "id": "zLqVB0kgev92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_n_grams(data, n=3):\n",
        "    descript = \"Generating \" + str(n) + \" Grams:\"\n",
        "\n",
        "    n_grams = [''.join(data[i:i+n]) for i in tqdm(range(len(data) - n + 1), desc=descript)]\n",
        "\n",
        "    return n_grams"
      ],
      "metadata": {
        "id": "0iqsefDGK40y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    data, hold_out = read_file('/content/gdrive/MyDrive/Colab_Notebooks/NLP/swahili/cwe-train.txt')\n",
        "\n",
        "    n_grams = gen_n_grams(data, GRAMS)\n",
        "    n_grams, mapping = encode(n_grams, data)\n",
        "\n",
        "    with open('/content/gdrive/MyDrive/Colab_Notebooks/NLP/swahili/cwmap.json', \"w\") as outfile:\n",
        "      json.dump(mapping, outfile)\n",
        "\n",
        "  # Below code from https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/#h2_7\n",
        "    vocab = len(mapping)\n",
        "    sequences = np.array(n_grams)\n",
        "    # create X and y\n",
        "    x, y = sequences[:, :-1], sequences[:, -1]\n",
        "    # one hot encode y\n",
        "    y = to_categorical(y, num_classes=vocab)\n",
        "    # create train and validation sets\n",
        "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=0.1, random_state=42)\n",
        "\n",
        "    print('Train shape:', x_tr.shape, 'Val shape:', x_val.shape)\n",
        "\n",
        "    model = build_model(vocab)\n",
        "\n",
        "  # Original addition\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=5)\n",
        "\n",
        "    model.fit(x_tr, y_tr, epochs=10, verbose=1, validation_data=(x_val, y_val), callbacks=stop_early, batch_size=500)\n",
        "    model.save('/content/gdrive/MyDrive/Colab_Notebooks/NLP/swahili/sw_act_model500')\n"
      ],
      "metadata": {
        "id": "T_2m3XwmKwSm"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GRAMS = 6"
      ],
      "metadata": {
        "id": "wmxZN9ZdLO5z"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMsCOFTvKzFn",
        "outputId": "1af3eca7-c979-4033-9719-1608eab73cad"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Generating 12 Grams::   0%|          | 0/181248 [00:00<?, ?it/s]\u001b[A\n",
            "Generating 12 Grams:: 100%|██████████| 181248/181248 [00:00<00:00, 1074916.95it/s]\n",
            "\n",
            "Encoding:   0%|          | 0/181248 [00:00<?, ?it/s]\u001b[A\n",
            "Encoding:  35%|███▌      | 64116/181248 [00:00<00:00, 641125.45it/s]\u001b[A\n",
            "Encoding: 100%|██████████| 181248/181248 [00:00<00:00, 633057.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (163123, 11) Val shape: (18125, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "Hi9nNZNoe4GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_entropy(preds, mapping, sequences):\n",
        "    entropy = 0\n",
        "\n",
        "    count = 0\n",
        "    pred_len = len(preds)\n",
        "\n",
        "    keys = sequences[:, -1]\n",
        "\n",
        "    for i in tqdm(range(0, len(sequences)), desc='Calculating Entropy'):\n",
        "      entropy -= (1/(pred_len)) * log2(preds[i][keys[i]])\n",
        "\n",
        "    return entropy"
      ],
      "metadata": {
        "id": "FdaJ8Y9nb4lk"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original"
      ],
      "metadata": {
        "id": "qe4vrjL5e5hT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_my_model():\n",
        "    model = keras.models.load_model('/content/gdrive/MyDrive/Colab_Notebooks/NLP/swahili/sw_act_model500')\n",
        "\n",
        "    data = read_test_data('/content/gdrive/MyDrive/Colab_Notebooks/NLP/swahili/sw-test.txt')\n",
        "\n",
        "    n_grams = gen_n_grams(data, GRAMS-1)\n",
        "    n_grams, mapping = encode(n_grams, data, loader=True)\n",
        "\n",
        "    vocab = len(mapping)\n",
        "    sequences = np.array(n_grams)\n",
        "\n",
        "    preds = model.predict(sequences)\n",
        "\n",
        "    entropy = calc_entropy(preds, mapping, sequences)\n",
        "    print('\\n', entropy)\n",
        "    "
      ],
      "metadata": {
        "id": "n2e5zjJbX4Sw"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_my_model()"
      ],
      "metadata": {
        "id": "RWVPdo1DZaHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ccda4c3-259f-462b-d931-125bd515caa2"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Generating 5 Grams::   0%|          | 0/3451378 [00:00<?, ?it/s]\u001b[A\n",
            "Generating 5 Grams::   4%|▎         | 127944/3451378 [00:00<00:02, 1279377.16it/s]\u001b[A\n",
            "Generating 5 Grams::   8%|▊         | 261688/3451378 [00:00<00:02, 1313502.35it/s]\u001b[A\n",
            "Generating 5 Grams::  12%|█▏        | 397253/3451378 [00:00<00:02, 1332718.97it/s]\u001b[A\n",
            "Generating 5 Grams::  15%|█▌        | 534045/3451378 [00:00<00:02, 1346590.73it/s]\u001b[A\n",
            "Generating 5 Grams::  20%|█▉        | 675727/3451378 [00:00<00:02, 1371893.24it/s]\u001b[A\n",
            "Generating 5 Grams::  24%|██▎       | 814885/3451378 [00:00<00:01, 1378547.89it/s]\u001b[A\n",
            "Generating 5 Grams::  28%|██▊       | 952740/3451378 [00:00<00:01, 1339477.94it/s]\u001b[A\n",
            "Generating 5 Grams::  31%|███▏      | 1086904/3451378 [00:00<00:01, 1290809.37it/s]\u001b[A\n",
            "Generating 5 Grams::  35%|███▌      | 1216408/3451378 [00:00<00:01, 1289417.95it/s]\u001b[A\n",
            "Generating 5 Grams::  39%|███▉      | 1345636/3451378 [00:01<00:01, 1262469.13it/s]\u001b[A\n",
            "Generating 5 Grams::  43%|████▎     | 1473861/3451378 [00:01<00:01, 1268287.91it/s]\u001b[A\n",
            "Generating 5 Grams::  47%|████▋     | 1604908/3451378 [00:01<00:01, 1280785.25it/s]\u001b[A\n",
            "Generating 5 Grams::  50%|█████     | 1733154/3451378 [00:01<00:01, 1278833.43it/s]\u001b[A\n",
            "Generating 5 Grams::  54%|█████▍    | 1861152/3451378 [00:01<00:01, 1195534.50it/s]\u001b[A\n",
            "Generating 5 Grams::  57%|█████▋    | 1981799/3451378 [00:01<00:01, 1172133.41it/s]\u001b[A\n",
            "Generating 5 Grams::  61%|██████    | 2104833/3451378 [00:01<00:01, 1188673.72it/s]\u001b[A\n",
            "Generating 5 Grams::  64%|██████▍   | 2224325/3451378 [00:01<00:01, 1176807.79it/s]\u001b[A\n",
            "Generating 5 Grams::  68%|██████▊   | 2343346/3451378 [00:01<00:00, 1180675.08it/s]\u001b[A\n",
            "Generating 5 Grams::  72%|███████▏  | 2468402/3451378 [00:01<00:00, 1201118.63it/s]\u001b[A\n",
            "Generating 5 Grams::  75%|███████▌  | 2588784/3451378 [00:02<00:00, 1158406.17it/s]\u001b[A\n",
            "Generating 5 Grams::  79%|███████▊  | 2716672/3451378 [00:02<00:00, 1193172.65it/s]\u001b[A\n",
            "Generating 5 Grams::  82%|████████▏ | 2841601/3451378 [00:02<00:00, 1209539.86it/s]\u001b[A\n",
            "Generating 5 Grams::  86%|████████▌ | 2962920/3451378 [00:02<00:00, 1207906.01it/s]\u001b[A\n",
            "Generating 5 Grams::  89%|████████▉ | 3085475/3451378 [00:02<00:00, 1213103.72it/s]\u001b[A\n",
            "Generating 5 Grams::  93%|█████████▎| 3206971/3451378 [00:02<00:00, 1207832.87it/s]\u001b[A\n",
            "Generating 5 Grams:: 100%|██████████| 3451378/3451378 [00:02<00:00, 1242108.81it/s]\n",
            "\n",
            "Encoding:   0%|          | 0/3451378 [00:00<?, ?it/s]\u001b[A\n",
            "Encoding:   1%|          | 36962/3451378 [00:00<00:09, 369604.49it/s]\u001b[A\n",
            "Encoding:   3%|▎         | 117278/3451378 [00:00<00:05, 624608.56it/s]\u001b[A\n",
            "Encoding:   6%|▌         | 196365/3451378 [00:00<00:04, 700506.72it/s]\u001b[A\n",
            "Encoding:   8%|▊         | 282112/3451378 [00:00<00:04, 762443.54it/s]\u001b[A\n",
            "Encoding:  11%|█         | 363364/3451378 [00:00<00:03, 780482.13it/s]\u001b[A\n",
            "Encoding:  13%|█▎        | 444702/3451378 [00:00<00:03, 791653.06it/s]\u001b[A\n",
            "Encoding:  15%|█▌        | 532654/3451378 [00:00<00:03, 820361.24it/s]\u001b[A\n",
            "Encoding:  18%|█▊        | 614691/3451378 [00:00<00:03, 816285.21it/s]\u001b[A\n",
            "Encoding:  20%|██        | 703557/3451378 [00:00<00:03, 838814.11it/s]\u001b[A\n",
            "Encoding:  23%|██▎       | 787450/3451378 [00:01<00:03, 838090.66it/s]\u001b[A\n",
            "Encoding:  25%|██▌       | 871267/3451378 [00:01<00:03, 836711.69it/s]\u001b[A\n",
            "Encoding:  28%|██▊       | 956732/3451378 [00:01<00:02, 842138.65it/s]\u001b[A\n",
            "Encoding:  30%|███       | 1041468/3451378 [00:01<00:02, 843702.35it/s]\u001b[A\n",
            "Encoding:  33%|███▎      | 1126325/3451378 [00:01<00:02, 845153.22it/s]\u001b[A\n",
            "Encoding:  35%|███▌      | 1210844/3451378 [00:01<00:02, 838758.29it/s]\u001b[A\n",
            "Encoding:  38%|███▊      | 1297797/3451378 [00:01<00:02, 847918.20it/s]\u001b[A\n",
            "Encoding:  40%|████      | 1384777/3451378 [00:01<00:02, 854455.80it/s]\u001b[A\n",
            "Encoding:  43%|████▎     | 1470236/3451378 [00:01<00:02, 852731.09it/s]\u001b[A\n",
            "Encoding:  45%|████▌     | 1558762/3451378 [00:01<00:02, 862448.21it/s]\u001b[A\n",
            "Generating 12 Grams::  39%|███▉      | 4606248/11771990 [15:13<23:40, 5044.84it/s]   \n",
            "\n",
            "Encoding:  50%|█████     | 1730696/3451378 [00:03<00:07, 233517.53it/s]\u001b[A\n",
            "Encoding:  53%|█████▎    | 1823136/3451378 [00:03<00:05, 305398.93it/s]\u001b[A\n",
            "Encoding:  56%|█████▌    | 1928713/3451378 [00:03<00:03, 402496.01it/s]\u001b[A\n",
            "Encoding:  58%|█████▊    | 2017166/3451378 [00:03<00:03, 477698.04it/s]\u001b[A\n",
            "Encoding:  61%|██████    | 2106578/3451378 [00:03<00:02, 553588.19it/s]\u001b[A\n",
            "Encoding:  64%|██████▎   | 2192025/3451378 [00:03<00:02, 612343.42it/s]\u001b[A\n",
            "Encoding:  66%|██████▌   | 2276801/3451378 [00:04<00:03, 324192.51it/s]\u001b[A\n",
            "Encoding:  68%|██████▊   | 2356445/3451378 [00:04<00:02, 388566.26it/s]\u001b[A\n",
            "Encoding:  71%|███████   | 2437824/3451378 [00:04<00:02, 457874.07it/s]\u001b[A\n",
            "Encoding:  73%|███████▎  | 2514544/3451378 [00:04<00:01, 516291.99it/s]\u001b[A\n",
            "Encoding:  75%|███████▌  | 2596364/3451378 [00:04<00:01, 580436.44it/s]\u001b[A\n",
            "Encoding:  78%|███████▊  | 2679961/3451378 [00:04<00:01, 639925.15it/s]\u001b[A\n",
            "Encoding:  80%|████████  | 2763314/3451378 [00:04<00:00, 688243.76it/s]\u001b[A\n",
            "Encoding:  83%|████████▎ | 2851955/3451378 [00:04<00:00, 740321.99it/s]\u001b[A\n",
            "Encoding:  85%|████████▌ | 2934454/3451378 [00:05<00:01, 320603.32it/s]\u001b[A\n",
            "Encoding:  87%|████████▋ | 3014179/3451378 [00:05<00:01, 387677.66it/s]\u001b[A\n",
            "Encoding:  90%|████████▉ | 3097259/3451378 [00:05<00:00, 461622.46it/s]\u001b[A\n",
            "Encoding:  92%|█████████▏| 3172573/3451378 [00:05<00:00, 517850.91it/s]\u001b[A\n",
            "Encoding:  94%|█████████▍| 3253290/3451378 [00:05<00:00, 580347.01it/s]\u001b[A\n",
            "Encoding:  97%|█████████▋| 3333686/3451378 [00:05<00:00, 633075.12it/s]\u001b[A\n",
            "Encoding: 100%|██████████| 3451378/3451378 [00:06<00:00, 571678.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "107856/107856 [==============================] - 200s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Entropy: 100%|██████████| 3451378/3451378 [00:02<00:00, 1186457.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 8.066807988816791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of models & Performance (KWERE) (12 grams)\n",
        "  # act_model - 1.235 entropy (Batch size 250)\n",
        "  # act_model500 - 1.282 entropy (Batch size 500)\n",
        "  # act_model125 - 1.233 entropy (Batch size 125)\n",
        "  # act_model50 - 1.201 entropy (Batch size 50)"
      ],
      "metadata": {
        "id": "4UudaUAbfQDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of models & Performance (SWAHILI) (6 grams)\n",
        "  # act_model - \n",
        "  # sw_act_model500 - 1.474 entropy (Batch size 500)\n",
        "  # act_model125 - \n",
        "  # act_model50 - "
      ],
      "metadata": {
        "id": "r-E11F-apqby"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}